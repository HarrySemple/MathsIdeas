<?xml version='1.0' encoding='utf-8'?>

<pretext xml:lang="en-US" xmlns:xi="http://www.w3.org/2001/XInclude">

  <docinfo>
    <macros>
      \newcommand{\R}{\mathbb R}
      \newcommand\myatop[2]{{{#1}\atop#2}} 
    </macros>
    <latex-image-preamble>
      \usepackage{tikz}
    </latex-image-preamble>
  </docinfo>

  <book xml:id="my-great-book">
    <title>Highlights</title>
    <subtitle>Maths I've Found Interesting</subtitle>

    <frontmatter xml:id="frontmatter">
      <titlepage>
        <author>
          <personname>Harry Semple</personname>
          <department>4th Year MMath</department>
          <institution>University of Bath</institution>
        </author>
        <date>
          <today />
        </date>
      </titlepage>

      <preface>
        <p>
          I am an MMath student at the University of Bath interested in PDE's, Machine Learning, Mathematical Biology and Scientific Computing. 
          This webpage is dedicated to satisfying results I've come across - both in and outside my degree.
        </p>
        <p>
          As of August 2023, much of this is still to be typeset; most topics have scanned placeholders until I get good at PreTeXt.
        </p>

        <figure>
          <caption>Website author with a german <url href="https://en.wikipedia.org/wiki/Swan">swan</url></caption>
          <image source="swan2.jpg" width="40%" />
        </figure>
      </preface>
    </frontmatter>

    <chapter xml:id="a-level">
      <title>A-Level</title>
      <introduction>
        <p>
          These are topics that are more accessible to sixth-form students
        </p>
      </introduction>

      <section xml:id="arc-length">
        <title>Arc Length</title>
        <image source="AL1.jpg" width="90%">
        </image>
      </section>


      <section xml:id="normal-1">
        <title>The Normal Distribution</title>
        <image source="ND1.jpg" width="90%">
        </image>
        <image source="ND2.jpg" width="90%">
        </image>
        <image source="ND3.jpg" width="90%">
        </image>
        <image source="ND4.jpg" width="90%">
        </image>
      </section>

      <section xml:id="guass">
        <title>More Uses of the Guassian Integral</title>
        <definition>
          <p>
            The Gamma Function is given by:
            <me>
              \Gamma (t)=\int^\infty_0 x^{t-1}e^{-x}dx
            </me>
          </p>
        </definition>
        <claim>
          <p>
            <m>\big(-\frac{1}{2}\big)!=\sqrt \pi</m>
          </p>
        </claim>
        <proof>
          <p>
            It follows from the definition of the Gamma Function that
            <md>
              <mrow>\Gamma(t+1)\amp=\int_0^\infty x^t e^{-x}</mrow>
              <mrow> \amp=\big[-x^t e^{-x}\big]_0^\infty+\int_0^\infty tx^{t-1}e^{-x}dx</mrow>
              <mrow> \amp=t\Gamma(t) </mrow>
              <mrow>\text{Thus, we have that}</mrow>
            <!--</md>
            Thus, we have that
            <md>-->
              <mrow>\Gamma(t)\amp=(t-1)\Gamma(t-1)=\cdots=(t-1)!\Gamma(1)</mrow>
            <!--</md>
            Where
            <md>-->
              <mrow>\text{Where}</mrow>
              <mrow> \Gamma(1)\amp=\int_0^\infty x^0e^{-x}dx</mrow>
              <mrow> \amp=\big[-e^{-x}\big]_0^\infty=1 </mrow>
              <mrow> \amp </mrow>
              <mrow> \Rightarrow\Gamma(t)\amp=(t-1)!</mrow>
              <mrow> \amp </mrow>
              <mrow> \Rightarrow \big(-\frac{1}{2}\big)!\amp=\Gamma\big(\frac{1}{2}\big) </mrow>
              <mrow> \amp=\int_0^\infty x^{1/2}e^{-x}dx</mrow>
              <mrow> \amp=2\int_0^\infty e^{-x^2}dx</mrow>
              <mrow> \amp=\int_{-\infty}^\infty e^{-x^2}dx = \sqrt\pi</mrow>
            </md>

          </p>
        </proof>
        <!--<image source="GI1.jpg" width="90%">
        </image>-->
      </section>

      <section xml:id="special-1">
        <title>Special Relativity</title>
        <image source="SP1.jpg" width="90%">
        </image>
      </section>
    </chapter>
    
    <chapter xml:id="numerical">
      <title>Numerical Analysis</title>

      <section xml:id="interpolation">
        <title>**Interpolation Rules</title>
      </section>

      <section xml:id="normal-2">
        <title>**Estimating The Normal Distribution</title>
      </section>

    </chapter>

    <chapter xml:id="ch-lin-alg">
      <title>Linear Algebra and ODE's</title>
      <section xml:id="sec-eig">
        <title>Eigenvalues and Eigenvectors</title>
        <definition xml:id="def-eigenvalue">
          <statement>
            <p>
              Given a matrix <m>A\in\mathbb{R}^{N\times N}</m>,
              <md>
                <mrow> \lambda\in\mathbb{C}\text{ is an eigenvalue of }A\amp\iff\exists v\in\mathbb{R}^N \text{ s.t. } Av = \lambda v </mrow>
                <mrow> v\in\mathbb{R}^N\text{ is an eigenvector of }A\amp\iff\exists \lambda\in\mathbb{C} \text{ s.t. } Av = \lambda v </mrow>
              </md>
            </p>
          </statement>
        </definition>
      </section>
      <section xml:id="sec-Eigenfunctions">
        <title>**Eigenfunctions</title>
        
      </section>
    </chapter>

    <chapter xml:id="mult-calc">
      <title>Multivariate Calculus and PDE's</title>

      <section xml:id="key-results">
        <title>**Key Results</title>
      </section>

      <section xml:id="math-bio-1">
        <title>Uses In Mathematical Biology</title>
        <subsection xml:id="subsec-mb1">
          <title>Derivation of the Conservation Equation</title>
          <p>
            Consider the conservation of particles in an arbitrary volume <m>V</m>, enclosed by a surface <m>S</m>, within a smooth domain <m>\Omega</m>. By the law of conservation, we have that:
            <men xml:id="eqn-pre-conservation">
              \myatop{\text{Particles in }V}{\text{@ }t+\delta t} = \myatop{\text{Particles in }V}{\text{@ }t} + \myatop{\text{Net particles}}{\text{entering }V} + \myatop{\text{Net particles}}{\text{created in }V}
            </men>
            Now set
            <md>
              <mrow>u(x,t) =\amp\text{ Density of particles} </mrow>
              <mrow>f(x,t) =\amp\text{ Rate of change of particle density from sinks/sources} </mrow>
              <mrow>J(x,t) =\amp\text{ Flux of particles} </mrow>
            </md>
            By <xref ref="eqn-pre-conservation"/>, we have
            <me>
              \int_{V}u(x,t+\delta t)dV = \int_{V}u(x,t)dV - \delta t\int_{S}J(x,t)\cdot dS + \delta t\int_{V}f(x,t)dV
            </me>
            Applying the divergence theorem and rearranging gives us
            <me>
              \int_{V}\frac{u(x,t+\delta t)-u(x,t)}{\delta t}+\nabla\cdot J(x,t)-f(x,t)dV=0
            </me>
            As <m>V</m> is an arbitrary volume within <m>\Omega</m>, we have that the integrand itself is zero. In addition, <m>\delta t</m> is arbitrary and is now sent to zero, giving us:
            <men xml:id="eqn-conservation">
              \frac{\partial u}{\partial t} + \nabla\cdot J-f=0
            </men>
            The Conservation Equation.
          </p>
        </subsection>
        <subsection xml:id="subsec-mb2">
          <title>Advective and Diffusive Flux</title>
          <p>
            In the context of biological models, we can often split <m>J</m> into two components:
            <me>
              J = J_\text{adv}+J_\text{diff}
            </me>
            Where 
            <md>
              <mrow> J_\text{adv}\amp=u(x,t)v(x,t) </mrow>
              <mrow> \text{i.e. particles @ }(x,t)\amp\text{ are moved along a vector }v\text{ due to some} </mrow>
              <mrow>  \text{advection property in the}\amp\text{ system - such as active trasport within a cell} </mrow>
              <mrow> \amp </mrow>
              <mrow> J_\text{diff}\amp=-D\nabla u(x,t) </mrow>
              <mrow> \text{i.e. by Fick's Law, particles}\amp\text{ move down their concentration gradient}</mrow>
            </md>
            Thus, the Advection-Diffusion Equation is given by:
            <men xml:id="eqn-adv-diff">
              \frac{\partial u}{\partial t}=-\nabla\cdot (vu)+\nabla\cdot (D\nabla u) + f
            </men>
          </p>
          <subsection xml:id="subsec-taxis">
            <title>Taxis Terms for the Flux</title>
            <p>
              Now we model movement in response to an external cue - such as light, tempreture, chemicals or another population. 
              This results in an advective flux specified by:
              <me>
                J=\chi u\nabla c
              </me>
              Where <m>c=</m> concentration of the external cue.
            </p>
          </subsection>
        </subsection>
      </section>

      <section xml:id="theory-pdes">
        <title>**Theory of PDE's</title>
      </section>

      <section xml:id="num-pdes">
        <title>**Numerical Solutions of Elliptic PDE's</title>
      </section>

    </chapter>

    <chapter xml:id="ch-ml">
      <title>Deep Learning</title>
      <section xml:id="sec-ml-intro">
        <title>Introduction</title>
        <definition xml:id="def-sigmoid">
          <statement>
            <p>
              A common activation function is the sigmoid function, given by 
              <me>\sigma (x)=\frac{1}{1+e^{-x}}</me>
              for <m>x\in\mathbb{R}</m>.
            </p>
            <p>
              In future, this will be applied to vectors component-wise. That is, for <m>z\in\mathbb{R}^n</m>
              <me>\sigma(z)_i=\sigma(z_i)</me>.
            </p>
          </statement>
        </definition>
        <remark>
          <p>
            It can be easily checked that <m>\sigma '(x) = \sigma(x)(1-\sigma(x))</m>
          </p>
        </remark>
        <p>
          Now we set up layers of neurons. Each neuron computes a single real number and passes it to every neuron in the next layer.
          In turn, each new neuron performs its own weighted combination of these values, adds its own bias and applies the sigmoid function.
          This is illustrated by
          <me>\myatop{a}{\text{Previous output}} \myatop{\longrightarrow}{} \myatop{\sigma(Wa+b)}{\text{Next output}}</me>
          <md>
            <mrow>a\amp \longrightarrow\amp \amp\sigma(Wa + b)</mrow>
            <mrow>\text{Previous }\amp \text{output} \amp \amp \text{Next output} </mrow>
          </md>
          Where <m>a,b\in \mathbb{R}^{\text{prev. #neurons}}</m> and <m>W\in\mathbb{R}^{\text{next #neurons }\times\text{ prev. #neurons}}</m>.
          
        </p>
        <remark>
          <p>
            The <m>i^\text{th}</m> next neuron outputs <m>\sigma(\sum_j w_{ij}a_j+b_i)</m>
          </p>
        </remark>
        <subsection xml:id="subsec-ml1">
          <title>General Setup</title>
          <p>
            'Deep learning' refers to the use of hidden layers between the input and output layers.
            For a network of <m>L</m> layers, <m>1\text{ and }L</m> are the respective input, output layers.
            Each layer <m>l\in\{ 1, \dots, L\}</m> contains <m>n_l</m> neurons. 
            Thus, the netwrok maps <m>\mathbb{R}^{n_1}\rightarrow\mathbb{R}^{n_L}</m>.
            <md>
              <mrow>W^{[l]} \amp\in\mathbb{R}^{n_l\times n_{l-1}} </mrow>
              <mrow>a^{[l]},b^{[l]} \amp\in\mathbb{R}^{n_l} </mrow>
            </md>
            Where
            <md>
              <mrow>a^{[1]} \amp=x </mrow>
              <mrow>a^{[l]} \amp=\sigma(W^{[l]}a^{[l-1]} + b^{[l]}) </mrow>
            </md>
            For <m>N</m> training datapoints,
            <me>
              \text{Cost } = \frac{1}{N}\sum_{i=1}^N \frac{1}{2} ||y(x^{\{i\}})-a^{[L]}(x^{\{i\}})||_2^2
            </me>
            
          </p>
        </subsection>
      </section>
      <section xml:id="sec-stochastic">
        <title>Stochastic Gradient</title>
        <p>
          Building a network corresponds to choosing parameters (of <m>W^{[l]}</m> and <m>b^{[l]}</m>) that minimise the the cost function.
          Taking <m>s</m> to be the number of parameters, we store these variables in a single vector <m>p\in\mathbb{R}^s</m>. 
        </p>
        <p>
          By a taylor series expansion, we have
          <md>
            <mrow>\text{Cost}(p+\Delta p) \amp\approx \text{Cost}(p)+\sum_{r=1}^s\frac{\partial\text{Cost}(p)}{\partial p_r}\Delta p_r </mrow>
            <mrow> \amp=\text{Cost}(p) + \nabla\text{Cost}(p)^T\Delta p </mrow>
          </md> 
          Which motivates us to make <m>\nabla\text{Cost}(p)^T\Delta p </m> as negative as possible. Cauchy-Schwarz tells us that <m>|f^Tg|\leq||f||_2||g||_2</m>,
          so this happens when <m>f^Tg = -||f||_2||g||_2 \iff f = -\lambda g</m>. 
          Thus, we choose <m>\Delta p = -\eta\nabla\text{Cost}(p)</m>
          and update accoringly:
          <me>
            p\longrightarrow p-\eta\nabla\text{Cost}(p)
          </me>
          Iterating this update forms the basis of gradient descent, with <m>\eta</m> as the learning rate. 
        </p>
        <remark>
            <p>
              Computing <m>\nabla\text{Cost}(p)</m> is expensive. 
              It is formulated as:
              <me>
                \nabla\text{Cost}(p)=\frac{1}{N}\sum_{i=1}^N\nabla C_{x^\{i\}}(p)
              </me>
              
            </p>
        </remark>
        <algorithm xml:id="alg-sto-grad">
          <statement>
            <p>
              The Stochastic Gradient Algorithm
            </p>
          </statement>
        </algorithm>

      </section>
      <section xml:id="sec-auto-diff">
        <title>**Automatic Differentiation</title>
        
      </section>
    </chapter>

    <backmatter xml:id="backmatter">
      <title>Backmatter</title>

      <section xml:id="misc-1">
        <title>Miscellaneous</title>
        <subsection xml:id="subsec-Natural-Logorithm">
          <title>Natural Logorithm</title>
          <image source="M1.jpg" width="90%">
          </image>
        </subsection>
        <subsection xml:id="subsec-Irrational-Indices">
          <title>Irrational Indices</title>
          <image source="M2.jpg" width="90%">
          </image>
        </subsection>
        <subsection xml:id="subsec-Fourier-Series">
          <title>Fourier Series</title>
          <image source="F1.jpg" width="90%">
          </image>
          <image source="F2.jpg" width="90%">
          </image>
          <image source="F3.jpg" width="90%">
          </image>
          <image source="F4.jpg" width="90%">
          </image>
        </subsection>
      </section>

      <colophon>
        <p> This book was authored in <pretext />. </p>

        <copyright>
          <year>2023<ndash />2024</year>
          <holder>Harry Semple</holder>
          <shortlicense> 
            This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit <url href="http://creativecommons.org/licenses/by-sa/4.0/" visual="creativecommons.org/licenses/by-sa/4.0"> CreativeCommons.org</url>
          </shortlicense>
        </copyright>
      </colophon>

    </backmatter>

  </book>
</pretext>
